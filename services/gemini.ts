import { GoogleGenAI, Modality, Type } from "@google/genai";
import { ReadingMode, VoiceEmotion, AdvancedVoiceSettings } from "../types";
import { VIETNAMESE_ABBREVIATIONS } from "../constants";

const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

/**
 * Ki·ªÉm tra API Key c√≥ th·ª±c s·ª± ho·∫°t ƒë·ªông hay kh√¥ng b·∫±ng m·ªôt request t·ªëi gi·∫£n
 */
export const testApiKey = async (apiKey: string): Promise<{ valid: boolean, message: string }> => {
  try {
    const ai = new GoogleGenAI({ apiKey });
    // Th·ª≠ g·ªçi m·ªôt l·ªánh generateContent si√™u ng·∫Øn ƒë·ªÉ check key
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: "Ping",
      config: { maxOutputTokens: 1 }
    });
    if (response) return { valid: true, message: "API Key ho·∫°t ƒë·ªông t·ªët." };
    return { valid: false, message: "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI." };
  } catch (error: any) {
    const info = handleAiError(error);
    return { valid: false, message: info.message };
  }
};

/**
 * X·ª≠ l√Ω l·ªói Gemini API chi ti·∫øt
 * H·ªó tr·ª£ ƒë·∫ßy ƒë·ªß: rate limit (429), quota exhausted, overload (503), v√† c√°c l·ªói kh√°c
 */
export const handleAiError = (error: any): { message: string, isRateLimit: boolean, shouldWait: boolean, isOverload: boolean } => {
  const rawMessage = error?.message ? String(error.message) : String(error);
  const lowerMessage = rawMessage.toLowerCase();
  
  // Ki·ªÉm tra rate limit v√† quota exhausted
  const isRateLimit = lowerMessage.includes("429") || 
                      lowerMessage.includes("resource exhausted") || 
                      lowerMessage.includes("quota") ||
                      lowerMessage.includes("quota exhausted") ||
                      lowerMessage.includes("rate limit") ||
                      lowerMessage.includes("too many requests");
  
  // Ki·ªÉm tra overload v√† server errors
  const isOverload = lowerMessage.includes("503") ||
                     lowerMessage.includes("service unavailable") ||
                     lowerMessage.includes("overload") ||
                     lowerMessage.includes("over capacity") ||
                     lowerMessage.includes("model is overloaded") ||
                     lowerMessage.includes("server overload") ||
                     lowerMessage.includes("engine over capacity");
  
  // Ki·ªÉm tra invalid key
  const isInvalidKey = lowerMessage.includes("400") || 
                       lowerMessage.includes("401") || 
                       lowerMessage.includes("403") || 
                       lowerMessage.includes("api key") || 
                       lowerMessage.includes("invalid argument") || 
                       lowerMessage.includes("not found") ||
                       lowerMessage.includes("unauthenticated");
  
  // Ki·ªÉm tra safety block
  const isSafetyBlock = lowerMessage.includes("safety") || 
                        lowerMessage.includes("blocked");

  // Rate limit v√† quota exhausted - c·∫ßn retry v·ªõi delay
  if (isRateLimit) {
    return { 
      message: "‚ùå H·∫øt h·∫°n m·ª©c (429/Quota exhausted).", 
      isRateLimit: true, 
      shouldWait: true,
      isOverload: false
    };
  }
  
  // Overload - c·∫ßn retry v·ªõi delay l·ªõn h∆°n
  if (isOverload) {
    return { 
      message: "‚ö†Ô∏è Server qu√° t·∫£i (503/Overload).", 
      isRateLimit: true, // X·ª≠ l√Ω nh∆∞ rate limit ƒë·ªÉ c√≥ retry
      shouldWait: true,
      isOverload: true
    };
  }
  
  // Invalid key - kh√¥ng retry
  if (isInvalidKey) {
    return { 
      message: "üö´ Key kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ b·ªã v√¥ hi·ªáu h√≥a.", 
      isRateLimit: false, 
      shouldWait: false,
      isOverload: false
    };
  }
  
  // Safety block - kh√¥ng retry
  if (isSafetyBlock) {
    return { 
      message: "üõ°Ô∏è N·ªôi dung b·ªã ch·∫∑n do ch√≠nh s√°ch an to√†n.", 
      isRateLimit: false, 
      shouldWait: false,
      isOverload: false
    };
  }
  
  // L·ªói kh√°c
  return { 
    message: `‚ùó L·ªói: ${rawMessage.substring(0, 100)}`, 
    isRateLimit: false, 
    shouldWait: false,
    isOverload: false
  };
};

/**
 * B·ªò 1: CHU·∫®N H√ìA C∆† B·∫¢N B·∫∞NG QUY T·∫ÆC
 * - X·ª≠ l√Ω k√Ω hi·ªáu, ƒë∆°n v·ªã, ng√†y th√°ng, t·ª´ vi·∫øt t·∫Øt ph·ªï bi·∫øn
 * - Kh√¥ng thay ƒë·ªïi n·ªôi dung, ch·ªâ l√†m cho d·ªÖ ƒë·ªçc to h∆°n
 */
export const normalizeTextForSpeech = (text: string): string => {
  if (!text) return "";

  // 1. Chu·∫©n h√≥a Unicode (NFC) ƒë·ªÉ x·ª≠ l√Ω l·ªói font v√† d·∫•u ti·∫øng Vi·ªát
  let processed = text.normalize("NFC");
  processed = processed.replace(/[\u200B-\u200D\uFEFF]/g, " ");

  // 2. X·ª≠ l√Ω k√Ω hi·ªáu to√°n h·ªçc v√† so s√°nh (Tr√°nh ƒë·ªçc sai k√Ω hi·ªáu)
  processed = processed.replace(/(\d+)\s*%\b/g, "$1 ph·∫ßn trƒÉm");
  processed = processed.replace(/\b\+\b/g, " c·ªông ");
  processed = processed.replace(/\s=\s/g, " b·∫±ng ");
  processed = processed.replace(/\s>\s/g, " l·ªõn h∆°n ");
  processed = processed.replace(/\s<\s/g, " nh·ªè h∆°n ");
  processed = processed.replace(/\b(\d+)\s*\*\s*(\d+)\b/g, "$1 nh√¢n $2");
  
  // 3. X·ª≠ l√Ω ng√†y th√°ng chuy√™n s√¢u
  // dd/mm/yyyy -> ng√†y dd th√°ng mm nƒÉm yyyy
  processed = processed.replace(/\b(\d{1,2})\/(\d{1,2})\/(\d{4})\b/g, "ng√†y $1 th√°ng $2 nƒÉm $3");
  // dd/mm -> ng√†y dd th√°ng mm
  processed = processed.replace(/\b(\d{1,2})\/(\d{1,2})\b/g, "ng√†y $1 th√°ng $2");

  // 4. X·ª≠ l√Ω ƒë∆°n v·ªã ti·ªÅn t·ªá v√† ƒëo l∆∞·ªùng (Ch·ªâ khi ƒë·ª©ng sau s·ªë)
  const units: Record<string, string> = {
    "kg": "ki l√¥ gam", "km": "ki l√¥ m√©t", "cm": "xƒÉng ti m√©t", "mm": "mi li m√©t",
    "m2": "m√©t vu√¥ng", "m3": "m√©t kh·ªëi", "ml": "mi li l√≠t", "l": "l√≠t", "g": "gam",
    "ƒë": "ƒë·ªìng", "vnd": "vi·ªát nam ƒë·ªìng", "usd": "ƒë√¥ la m·ªπ", "tr": "tri·ªáu", "t·ª∑": "t·ª∑"
  };
  
  for (const [unit, reading] of Object.entries(units)) {
      const regex = new RegExp(`(\\d)\\s*${unit}\\b`, 'gi');
      processed = processed.replace(regex, `$1 ${reading}`);
  }

  // 4.1. X·ª≠ l√Ω ri√™ng m·ªôt s·ªë c·ª•m vi·∫øt t·∫Øt h√†nh ch√≠nh hay g·∫∑p nh∆∞ng c√≥ kho·∫£ng tr·∫Øng b√™n trong
  // V√≠ d·ª•: "UB MTTQ Vi·ªát Nam" -> "UBMTTQ Vi·ªát Nam" ƒë·ªÉ t·ª´ ƒëi·ªÉn m·ªü r·ªông ƒë√∫ng
  processed = processed.replace(/\bUB\s+MTTQ\b/gi, "UBMTTQ");

  // 4.2. S·ª≠a c√°c l·ªói ch√≠nh t·∫£ ph·ªï bi·∫øn trong vƒÉn b·∫£n h√†nh ch√≠nh
  // "u·ª∑" -> "·ªßy" (d·∫•u h·ªèi thay v√¨ d·∫•u ng√£)
  processed = processed.replace(/\bu·ª∑\b/gi, "·ªßy");
  processed = processed.replace(/\bƒê·∫£ng\s+u·ª∑\b/gi, "ƒê·∫£ng ·ªßy");
  processed = processed.replace(/\bƒë·∫£ng\s+u·ª∑\b/gi, "ƒë·∫£ng ·ªßy");
  // "H·ªôi ƒë·ªìng nh√¢n v√†" -> "H·ªôi ƒë·ªìng nh√¢n d√¢n" (s·ª≠a l·ªói thi·∫øu ch·ªØ)
  processed = processed.replace(/\bh·ªôi ƒë·ªìng nh√¢n v√†\b/gi, "H·ªôi ƒë·ªìng nh√¢n d√¢n");
  processed = processed.replace(/\bH·ªôi ƒë·ªìng nh√¢n v√†\b/g, "H·ªôi ƒë·ªìng nh√¢n d√¢n");

  // 5. M·ªü r·ªông t·ª´ vi·∫øt t·∫Øt (Theo danh s√°ch chu·∫©n t·ª´ constants)
  const sortedAbbrs = Object.keys(VIETNAMESE_ABBREVIATIONS).sort((a, b) => b.length - a.length);
  for (const abbr of sortedAbbrs) {
      const fullText = VIETNAMESE_ABBREVIATIONS[abbr];
      const escapedAbbr = abbr.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
      // N·∫øu c√≥ d·∫•u ch·∫•m ·ªü cu·ªëi (TP.), match nguy√™n vƒÉn, n·∫øu kh√¥ng d√πng word boundary.
      // D√πng 'gi' ƒë·ªÉ kh√¥ng ph√¢n bi·ªát hoa/th∆∞·ªùng, gi√∫p ƒë·ªçc ƒë√∫ng trong m·ªçi ki·ªÉu vƒÉn b·∫£n.
      const regex = abbr.endsWith('.') ? new RegExp(escapedAbbr, 'gi') : new RegExp(`\\b${escapedAbbr}\\b`, 'gi');
      processed = processed.replace(regex, fullText + " ");
  }

  // 6. Chu·∫©n h√≥a d·∫•u c√¢u ƒë·ªÉ AI ng·∫Øt ngh·ªâ ƒë√∫ng (D·∫•u c√¢u d√≠nh li·ªÅn)
  processed = processed.replace(/([,.!:;?])(?=[^\s\d])/g, '$1 '); // "ch√†o,b·∫°n" -> "ch√†o, b·∫°n"
  processed = processed.replace(/\s+([,.!:;?])/g, '$1'); // "ch√†o , b·∫°n" -> "ch√†o, b·∫°n"
  
  // 7. X·ª≠ l√Ω g·∫°ch ƒë·∫ßu d√≤ng v√† ph√¢n ƒëo·∫°n (Tr√°nh ƒë·ªçc l√† "tr·ª´")
  processed = processed.replace(/(^|\n)\s*-\s+/g, "$1, "); 

  // 8. D·ªçn d·∫πp kho·∫£ng tr·∫Øng th·ª´a
  return processed.replace(/\s+/g, ' ').trim();
};

/**
 * B·ªò 2: HI·ªÜU ƒê√çNH TH√îNG MINH B·∫∞NG AI
 * D√πng Gemini ƒë·ªÉ:
 *  - M·ªü r·ªông c√°c t·ª´ vi·∫øt t·∫Øt hi·∫øm g·∫∑p
 *  - S·ª≠a l·ªói ch√≠nh t·∫£, d·∫•u c√¢u
 *  - Gi·ªØ nguy√™n n·ªôi dung, kh√¥ng t√≥m t·∫Øt, kh√¥ng th√™m √Ω m·ªõi
 * Ph√π h·ª£p cho vƒÉn b·∫£n h√†nh ch√≠nh, vƒÉn b·∫£n d√†i v√† phong ph√∫.
 */
export const refineTextForReading = async (rawText: string, apiKey: string = "", onLog?: (m: string, t: 'info' | 'error') => void): Promise<string> => {
  const text = rawText || "";
  if (!text.trim()) return "";

  try {
    const ai = new GoogleGenAI({ apiKey });
    const prompt = `
B·∫°n l√† chuy√™n gia ng√¥n ng·ªØ ti·∫øng Vi·ªát chuy√™n nghi·ªáp, ƒë·ªçc b·∫£n tin th·ªùi s·ª± tr√™n truy·ªÅn h√¨nh VTV.

NHI·ªÜM V·ª§ QUAN TR·ªåNG:
1. S·ª¨A L·ªñI CH√çNH T·∫¢: ƒê·∫£m b·∫£o m·ªçi t·ª´ ƒë·ªÅu ƒë√∫ng ch√≠nh t·∫£ ti·∫øng Vi·ªát chu·∫©n. V√≠ d·ª•:
   - "u·ª∑" -> "·ªßy" (d·∫•u h·ªèi, kh√¥ng ph·∫£i d·∫•u ng√£)
   - "H·ªôi ƒë·ªìng nh√¢n v√†" -> "H·ªôi ƒë·ªìng nh√¢n d√¢n" (s·ª≠a l·ªói thi·∫øu ch·ªØ)
   - Ki·ªÉm tra v√† s·ª≠a m·ªçi l·ªói ch√≠nh t·∫£ kh√°c trong vƒÉn b·∫£n

2. M·ªû R·ªòNG T·ª™ VI·∫æT T·∫ÆT: M·ªü r·ªông T·∫§T C·∫¢ c√°c t·ª´ vi·∫øt t·∫Øt, k·ªÉ c·∫£:
   - HƒêND -> H·ªôi ƒë·ªìng nh√¢n d√¢n
   - UBND -> ·ª¶y ban nh√¢n d√¢n
   - UBMTTQ, UB MTTQ -> ·ª¶y ban M·∫∑t tr·∫≠n T·ªï qu·ªëc
   - BCH -> Ban ch·∫•p h√†nh
   - V√† m·ªçi t·ª´ vi·∫øt t·∫Øt kh√°c

3. CHU·∫®N H√ìA D·∫§U C√ÇU: Th√™m d·∫•u ch·∫•m, ph·∫©y ƒë√∫ng ch·ªó ƒë·ªÉ d·ªÖ ƒë·ªçc, ng·∫Øt ngh·ªâ t·ª± nhi√™n.

4. GI·ªÆ NGUY√äN N·ªòI DUNG: 
   - KH√îNG ƒë∆∞·ª£c t√≥m t·∫Øt
   - KH√îNG ƒë∆∞·ª£c l∆∞·ª£c b·ªè √Ω
   - KH√îNG th√™m b√¨nh lu·∫≠n hay √Ω ki·∫øn c√° nh√¢n
   - Gi·ªØ nguy√™n t√™n ng∆∞·ªùi, t√™n ƒë·ªãa danh, s·ªë li·ªáu

5. PHONG C√ÅCH: Vi·∫øt l·∫°i theo phong c√°ch ƒë·ªçc b·∫£n tin th·ªùi s·ª±: r√µ r√†ng, m·∫°ch l·∫°c, vƒÉn phong h√†nh ch√≠nh/trang tr·ªçng, GI·ªåNG ƒê·ªÄU, kh√¥ng l√™n xu·ªëng c·∫£m x√∫c qu√° m·ª©c.

VƒÉn b·∫£n g·ªëc c·∫ßn hi·ªáu ƒë√≠nh:
"""${text}"""

H√£y tr·∫£ v·ªÅ CH·ªà vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c s·ª≠a ch√≠nh t·∫£, m·ªü r·ªông vi·∫øt t·∫Øt, v√† chu·∫©n h√≥a d·∫•u c√¢u. KH√îNG k√®m gi·∫£i th√≠ch hay b√¨nh lu·∫≠n.`;

    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: prompt
    });

    const refined = (response as any)?.text || text;
    if (onLog) onLog("AI ƒë√£ hi·ªáu ƒë√≠nh vƒÉn b·∫£n ƒë·ªÉ ƒë·ªçc to ch√≠nh x√°c h∆°n.", "info");
    return refined.trim();
  } catch (error: any) {
    const info = handleAiError(error);
    if (onLog) onLog(`Kh√¥ng th·ªÉ hi·ªáu ƒë√≠nh b·∫±ng AI, d√πng nguy√™n vƒÉn b·∫£n g·ªëc. (${info.message})`, "warning" as any);
    // N·∫øu AI l·ªói, fallback: ch·ªâ d√πng normalizeTextForSpeech th√¥ng th∆∞·ªùng
    return text;
  }
};

export const generateContentFromDescription = async (prompt: string, modePrompt: string, onLog?: any, apiKey: string = "") => {
  try {
    const ai = new GoogleGenAI({ apiKey });
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: `${modePrompt}\n\n${prompt}\n\nY√™u c·∫ßu: Vi·∫øt ti·∫øng Vi·ªát chu·∫©n, tuy·ªát ƒë·ªëi kh√¥ng vi·∫øt t·∫Øt, kh√¥ng d√πng ti·∫øng l√≥ng.`,
    });
    return response.text || '';
  } catch (error: any) { throw new Error(handleAiError(error).message); }
};

export const generateAudioSegment = async (text: string, config: any, onLog?: any, apiKey: string = ""): Promise<ArrayBuffer> => {
  try {
    const ai = new GoogleGenAI({ apiKey });
    // ƒê·∫£m b·∫£o voiceName lu√¥n ƒë∆∞·ª£c truy·ªÅn ƒë√∫ng v√† nh·∫•t qu√°n
    const voiceName = config?.voiceName || 'Kore'; // Fallback n·∫øu kh√¥ng c√≥
    
    // Log ƒë·ªÉ debug n·∫øu c·∫ßn
    if (onLog && text.length > 100) {
      onLog(`T·∫°o audio v·ªõi gi·ªçng: ${voiceName}, ƒë·ªô d√†i: ${text.length} k√Ω t·ª±`, "info");
    }
    
    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text }] }],
      config: {
        responseModalities: [Modality.AUDIO],
        speechConfig: { 
          voiceConfig: { 
            prebuiltVoiceConfig: { 
              voiceName: voiceName // ƒê·∫£m b·∫£o d√πng c√πng voiceName cho m·ªçi ƒëo·∫°n
            } 
          } 
        },
      },
    });
    const base64 = response.candidates?.[0]?.content?.parts?.find((p: any) => p.inlineData)?.inlineData?.data;
    if (!base64) throw new Error("TTS Failure");
    const binary = atob(base64);
    const bytes = new Uint8Array(binary.length);
    for (let i = 0; i < binary.length; i++) bytes[i] = binary.charCodeAt(i);
    return bytes.buffer;
  } catch (error: any) { throw new Error(handleAiError(error).message); }
};

export const generateAudioParallel = async (text: string, config: any, onProgress: any, onLog?: any, apiKey: string = ""): Promise<ArrayBuffer> => {
  const raw = text || "";

  // NGUY√äN T·∫ÆC TI·∫æT KI·ªÜM QUOTA V√Ä ƒê·∫¢M B·∫¢O CH√çNH T·∫¢:
  // - VƒÉn b·∫£n >= 500 k√Ω t·ª±: lu√¥n g·ªçi AI hi·ªáu ƒë√≠nh ƒë·ªÉ s·ª≠a ch√≠nh t·∫£, m·ªü r·ªông vi·∫øt t·∫Øt
  // - VƒÉn b·∫£n < 500 k√Ω t·ª±: v·∫´n g·ªçi AI n·∫øu c√≥ t·ª´ vi·∫øt t·∫Øt h√†nh ch√≠nh (HƒêND, UBND, UBMTTQ...)
  // - ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o m·ªçi vƒÉn b·∫£n h√†nh ch√≠nh ƒë·ªÅu ƒë∆∞·ª£c s·ª≠a ch√≠nh t·∫£ ƒë√∫ng
  const LONG_TEXT_THRESHOLD = 500;
  const hasAdministrativeAbbr = /\b(HƒêND|UBND|UB\s*MTTQ|BCH|ƒê·∫£ng\s*u·ª∑|ƒë·∫£ng\s*u·ª∑)\b/gi.test(raw);
  let preprocessedText = raw;

  if (raw.length >= LONG_TEXT_THRESHOLD || hasAdministrativeAbbr) {
    if (onLog) onLog("ƒêang nh·ªù AI hi·ªáu ƒë√≠nh ƒë·ªÉ s·ª≠a ch√≠nh t·∫£ v√† m·ªü r·ªông vi·∫øt t·∫Øt...", "info");
    preprocessedText = await refineTextForReading(raw, apiKey, onLog);
  }

  // B∆Ø·ªöC 2: Chu·∫©n h√≥a k·ªπ thu·∫≠t (k√Ω hi·ªáu, ƒë∆°n v·ªã, kho·∫£ng tr·∫Øng...) ƒë·ªÉ ƒë·ªçc TTS m∆∞·ª£t.
  const normalizedText = normalizeTextForSpeech(preprocessedText);

  // T·ªêI ∆ØU GI·ªÆ T√îNG GI·ªåNG TH·ªêNG NH·∫§T:
  // ∆Øu ti√™n ƒë·ªçc li·ªÅn m·ªôt ƒëo·∫°n ƒë·ªÉ tr√°nh ƒë·ªïi t√¥ng gi·ªçng (nam/n·ªØ) gi·ªØa c√°c ƒëo·∫°n.
  // Ng∆∞·ª°ng cao h∆°n (4000 k√Ω t·ª±) v√¨ vƒÉn b·∫£n sau khi AI m·ªü r·ªông vi·∫øt t·∫Øt s·∫Ω d√†i h∆°n nhi·ªÅu.
  // Ch·ªâ chia ƒëo·∫°n khi th·∫≠t s·ª± c·∫ßn thi·∫øt (vƒÉn b·∫£n si√™u d√†i > 4000 k√Ω t·ª±).
  const SINGLE_SEGMENT_THRESHOLD = 4000; // k√Ω t·ª± - tƒÉng cao ƒë·ªÉ ∆∞u ti√™n ƒë·ªçc li·ªÅn m·ªôt ƒëo·∫°n
  if (normalizedText.length <= SINGLE_SEGMENT_THRESHOLD) {
    if (onLog) onLog(`ƒê·ªçc li·ªÅn m·ªôt ƒëo·∫°n (${normalizedText.length} k√Ω t·ª±) ƒë·ªÉ gi·ªØ t√¥ng gi·ªçng th·ªëng nh·∫•t.`, "info");
    const buffer = await generateAudioSegment(normalizedText, config, onLog, apiKey);
    onProgress(100);
    return buffer;
  }
  
  // Ch·ªâ chia ƒëo·∫°n khi vƒÉn b·∫£n th·∫≠t s·ª± r·∫•t d√†i (> 4000 k√Ω t·ª±)
  if (onLog) onLog(`VƒÉn b·∫£n r·∫•t d√†i (${normalizedText.length} k√Ω t·ª±), chia th√†nh nhi·ªÅu ƒëo·∫°n nh∆∞ng v·∫´n gi·ªØ c√πng gi·ªçng ƒë·ªçc.`, "info");
  
  const rawChunks = normalizedText.match(/[^.!?\n]+[.!?\n]*|[^.!?\n]+/g) || [normalizedText];
  const combinedChunks: string[] = [];
  let current = "";
  const LIMIT = 600; 

  for (const c of rawChunks) {
    if ((current + c).length < LIMIT) current += c;
    else { if (current) combinedChunks.push(current.trim()); current = c; }
  }
  if (current) combinedChunks.push(current.trim());

  const results: ArrayBuffer[] = [];
  // ƒê·ªô tr·ªÖ ƒë·ªông gi·ªØa c√°c ƒëo·∫°n ƒë·ªÉ tr√°nh qu√° t·∫£i / h·∫øt quota
  // VƒÉn b·∫£n c√†ng d√†i -> delay c√†ng l·ªõn m·ªôt ch√∫t.
  const baseDelayMs = normalizedText.length > 3000 ? 2200 : normalizedText.length > 1500 ? 1600 : 1200;

  for (let i = 0; i < combinedChunks.length; i++) {
    if (i > 0) await delay(baseDelayMs); // Tr√°nh spam rate limit / quota
    const segment = await generateAudioSegment(combinedChunks[i], config, onLog, apiKey);
    results.push(segment);
    onProgress(Math.round(((i + 1) / combinedChunks.length) * 100));
  }

  const totalLength = results.reduce((acc, b) => acc + b.byteLength, 0);
  const finalBuffer = new Uint8Array(totalLength);
  let offset = 0;
  for (const res of results) {
    finalBuffer.set(new Uint8Array(res), offset);
    offset += res.byteLength;
  }
  return finalBuffer.buffer;
};

export const pcmToWav = (pcmBuffer: ArrayBuffer, sampleRate: number = 24000): Blob => {
  const length = pcmBuffer.byteLength;
  const buffer = new ArrayBuffer(44 + length);
  const view = new DataView(buffer);
  view.setUint32(0, 0x52494646, false); 
  view.setUint32(4, 36 + length, true); 
  view.setUint32(8, 0x57415645, false);
  view.setUint32(12, 0x666d7420, false); 
  view.setUint32(16, 16, true); 
  view.setUint16(20, 1, true);
  view.setUint16(22, 1, true); 
  view.setUint32(24, sampleRate, true); 
  view.setUint32(28, sampleRate * 2, true);
  view.setUint16(32, 2, true); 
  view.setUint16(34, 16, true); 
  view.setUint32(36, 0x64617461, false);
  view.setUint32(40, length, true); 
  new Uint8Array(buffer, 44).set(new Uint8Array(pcmBuffer));
  return new Blob([buffer], { type: 'audio/wav' });
};

export const pcmToMp3 = (pcmBuffer: ArrayBuffer, sampleRate: number = 24000): Blob => {
  const lamejs = (window as any).lamejs;
  if (!lamejs?.Mp3Encoder) return pcmToWav(pcmBuffer, sampleRate);
  const encoder = new lamejs.Mp3Encoder(1, sampleRate, 128);
  const samples = new Int16Array(pcmBuffer);
  const mp3Data = [];
  for (let i = 0; i < samples.length; i += 1152) {
    const chunk = samples.subarray(i, i + 1152);
    const mp3buf = encoder.encodeBuffer(chunk);
    if (mp3buf.length > 0) mp3Data.push(mp3buf);
  }
  const final = encoder.flush();
  if (final.length > 0) mp3Data.push(final);
  return new Blob(mp3Data, { type: 'audio/mp3' });
};

export const analyzeVoice = async (rawAudioBuffer: ArrayBuffer, onLog?: (m: string, t: 'info' | 'error') => void, apiKey: string = ""): Promise<any> => {
  const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
  const audioBuffer = await audioContext.decodeAudioData(rawAudioBuffer.slice(0));
  const durationToKeep = Math.min(audioBuffer.duration, 20);
  const framesToKeep = Math.floor(durationToKeep * audioBuffer.sampleRate);
  const newBuffer = audioContext.createBuffer(audioBuffer.numberOfChannels, framesToKeep, audioBuffer.sampleRate);
  for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
    newBuffer.getChannelData(i).set(audioBuffer.getChannelData(i).slice(0, framesToKeep));
  }
  const wavBlob = pcmToWav(audioBufferToWav(newBuffer), audioBuffer.sampleRate);
  const reader = new FileReader();
  const base64 = await new Promise<string>((resolve) => {
    reader.onloadend = () => resolve((reader.result as string).split(',')[1]);
    reader.readAsDataURL(wavBlob);
  });

  try {
    const ai = new GoogleGenAI({ apiKey });
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview', 
      contents: {
        parts: [
          { inlineData: { data: base64, mimeType: 'audio/wav' } },
          { text: `Analyze this audio. Return JSON: gender ("Nam"/"N·ªØ"), region ("B·∫Øc"/"Trung"/"Nam"), toneSummary (5 words), suggestedName (Vietnamese), description.` }
        ]
      },
      config: { responseMimeType: "application/json" }
    });
    
    let jsonText = response.text || "{}";
    jsonText = jsonText.replace(/^```json\s*/, '').replace(/\s*```$/, '');
    return JSON.parse(jsonText);
  } catch (e: any) {
    throw new Error(handleAiError(e).message);
  }
};

function audioBufferToWav(buffer: AudioBuffer): ArrayBuffer {
  const numChannels = buffer.numberOfChannels;
  const length = buffer.length * numChannels * 2;
  const result = new ArrayBuffer(length);
  const view = new DataView(result);
  const channels = [];
  for (let i = 0; i < numChannels; i++) channels.push(buffer.getChannelData(i));
  let offset = 0;
  for (let i = 0; i < buffer.length; i++) {
    for (let channel = 0; channel < numChannels; channel++) {
      let sample = channels[channel][i];
      sample = Math.max(-1, Math.min(1, sample));
      view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
      offset += 2;
    }
  }
  return result;
}

export const generateMarketingContent = async (imageBase64: string | null, description: string, onLog?: any, apiKey: string = "") => {
  try {
    const ai = new GoogleGenAI({ apiKey });
    const parts: any[] = [];
    if (imageBase64) parts.push({ inlineData: { mimeType: 'image/jpeg', data: imageBase64 } });
    parts.push({ text: `ƒê√≥ng vai chuy√™n gia marketing. D·ª±a tr√™n: "${description}", t·∫°o ti√™u ƒë·ªÅ (d∆∞·ªõi 10 t·ª´) v√† n·ªôi dung qu·∫£ng c√°o (30 t·ª´) h·∫•p d·∫´n. Tr·∫£ v·ªÅ JSON {title, content}.` });
    const response = await ai.models.generateContent({ model: 'gemini-3-flash-preview', contents: { parts }, config: { responseMimeType: "application/json" } });
    
    let jsonText = response.text || "{}";
    jsonText = jsonText.replace(/^```json\s*/, '').replace(/\s*```$/, '');
    return JSON.parse(jsonText);
  } catch (e: any) { throw new Error(handleAiError(e).message); }
};

export const generateAdImage = async (prompt: string, onLog?: any, apiKey: string = "") => {
  try {
    const ai = new GoogleGenAI({ apiKey });
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: [{ text: `High-quality advertising background: ${prompt}. No text.` }],
      config: { imageConfig: { aspectRatio: "1:1" } }
    });
    const part = response.candidates?.[0]?.content?.parts?.find(p => p.inlineData);
    if (!part) throw new Error("AI kh√¥ng tr·∫£ v·ªÅ ·∫£nh.");
    return `data:image/png;base64,${part.inlineData.data}`;
  } catch (e: any) {
    const errorInfo = handleAiError(e);
    // Th√™m th√¥ng tin chi ti·∫øt h∆°n cho rate limit v√† overload errors
    if (errorInfo.isRateLimit && !errorInfo.isOverload) {
      throw new Error(`${errorInfo.message} Key c√≥ th·ªÉ ƒë√£ h·∫øt quota cho model t·∫°o ·∫£nh.`);
    }
    if (errorInfo.isOverload) {
      throw new Error(`${errorInfo.message} Server ƒëang qu√° t·∫£i, vui l√≤ng th·ª≠ l·∫°i sau.`);
    }
    throw new Error(errorInfo.message);
  }
};

export const mixAudio = async (speechBuffer: ArrayBuffer, musicBuffer: ArrayBuffer, musicVolume: number): Promise<ArrayBuffer> => {
  const ctx = new (window.AudioContext || (window as any).webkitAudioContext)();
  
  // Decode both buffers
  const speech = await ctx.decodeAudioData(speechBuffer.slice(0));
  const music = await ctx.decodeAudioData(musicBuffer.slice(0));

  // Create OfflineAudioContext with the duration of the speech
  const offlineCtx = new OfflineAudioContext(1, speech.length, speech.sampleRate);

  // 1. Setup Speech Source
  const speechSource = offlineCtx.createBufferSource();
  speechSource.buffer = speech;
  speechSource.connect(offlineCtx.destination);
  speechSource.start(0);

  // 2. Setup Music Source (Looping to fit speech duration)
  const musicSource = offlineCtx.createBufferSource();
  musicSource.buffer = music;
  musicSource.loop = true; // Loop background music
  
  // 3. Apply Volume to Music
  const gainNode = offlineCtx.createGain();
  gainNode.gain.value = musicVolume; // 0.0 to 1.0
  
  musicSource.connect(gainNode);
  gainNode.connect(offlineCtx.destination);
  musicSource.start(0);

  // 4. Render
  const renderedBuffer = await offlineCtx.startRendering();
  
  // 5. Convert back to WAV ArrayBuffer
  return audioBufferToWav(renderedBuffer);
};
