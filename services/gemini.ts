import { GoogleGenAI, Modality, Type } from "@google/genai";
import { ReadingMode, VoiceEmotion, AdvancedVoiceSettings } from "../types";
import { VIETNAMESE_ABBREVIATIONS } from "../constants";

const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

/**
 * X·ª≠ l√Ω l·ªói Gemini API chi ti·∫øt ƒë·ªÉ ph·∫£n h·ªìi ng∆∞·ªùi d√πng r√µ r√†ng.
 * S·ª≠a l·ªói "substring is not a function" b·∫±ng c√°ch √©p ki·ªÉu chu·ªói an to√†n.
 */
export const handleAiError = (error: any): { message: string, isRateLimit: boolean, shouldWait: boolean } => {
  // ƒê·∫£m b·∫£o message lu√¥n l√† string nguy√™n th·ªßy
  const rawMessage = error?.message ? String(error.message) : String(error);
  const lowerMessage = rawMessage.toLowerCase();
  
  const isRateLimit = lowerMessage.includes("429") || lowerMessage.includes("resource exhausted") || lowerMessage.includes("quota");
  const isServerBusy = lowerMessage.includes("500") || lowerMessage.includes("503");
  const isInvalidKey = lowerMessage.includes("400") || lowerMessage.includes("401") || lowerMessage.includes("403") || lowerMessage.includes("api key") || lowerMessage.includes("invalid argument") || lowerMessage.includes("not found");
  const isSafetyBlock = lowerMessage.includes("safety") || lowerMessage.includes("blocked");

  if (isRateLimit) {
    return { 
      message: "‚ùå H·∫æT H·∫†N M·ª®C (QUOTA EXHAUSTED): T√†i kho·∫£n Google AI ƒë√£ ƒë·∫°t gi·ªõi h·∫°n y√™u c·∫ßu. Vui l√≤ng th·ª≠ l·∫°i sau 1-2 ph√∫t ho·∫∑c n√¢ng c·∫•p API Key.", 
      isRateLimit: true, 
      shouldWait: false 
    };
  }

  if (isServerBusy) {
    return { 
      message: "‚ö†Ô∏è M√ÅY CH·ª¶ AI ƒêANG QU√Å T·∫¢I: Google AI ƒëang x·ª≠ l√Ω qu√° nhi·ªÅu y√™u c·∫ßu. H·ªá th·ªëng s·∫Ω th·ª≠ l·∫°i sau v√†i gi√¢y...", 
      isRateLimit: false, 
      shouldWait: true 
    };
  }

  if (isInvalidKey) {
    return { 
      message: "üö´ L·ªñI API KEY: M√£ truy c·∫≠p AI kh√¥ng h·ª£p l·ªá, ƒë√£ b·ªã thu h·ªìi ho·∫∑c ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh h·ªá th·ªëng.", 
      isRateLimit: false, 
      shouldWait: false 
    };
  }

  if (isSafetyBlock) {
     return { 
       message: "üõ°Ô∏è N·ªòI DUNG B·ªä CH·∫∂N: VƒÉn b·∫£n vi ph·∫°m ch√≠nh s√°ch an to√†n c·ªßa Google AI (B·∫°o l·ª±c, th√π gh√©t, ho·∫∑c n·ªôi dung nh·∫°y c·∫£m). H√£y ch·ªânh s·ª≠a l·∫°i vƒÉn b·∫£n.", 
       isRateLimit: false, 
       shouldWait: false 
     };
  }
  
  return { 
    message: `‚ùó L·ªñI K·ª∏ THU·∫¨T: ${rawMessage.substring(0, 150) || "K·∫øt n·ªëi t·ªõi AI th·∫•t b·∫°i"}. Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng truy·ªÅn m·∫°ng.`, 
    isRateLimit: false, 
    shouldWait: false 
  };
};

/**
 * H√†m chu·∫©n h√≥a vƒÉn b·∫£n th√¥ng minh tr∆∞·ªõc khi ƒë·ªçc.
 * 1. Thay th·∫ø c√°c t·ª´ vi·∫øt t·∫Øt (UBND, HƒêND...) th√†nh c√¢u ƒë·∫ßy ƒë·ªß.
 * 2. Thay th·∫ø c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát nh∆∞ "-" th√†nh d·∫•u ph·∫©y ƒë·ªÉ AI ng·∫Øt ngh·ªâ ƒë√∫ng nh·ªãp.
 */
const normalizeTextForSpeech = (text: string): string => {
  let processed = text;

  // 1. Thay th·∫ø d·∫•u g·∫°ch ngang gi·ªØa c√°c c·ª•m t·ª´ (VD: ƒê·∫£ng u·ª∑- HƒêND) th√†nh d·∫•u ph·∫©y ƒë·ªÉ t·∫°o nh·ªãp ngh·ªâ
  // Regex n√†y t√¨m d·∫•u g·∫°ch ngang c√≥ kho·∫£ng tr·∫Øng xung quanh ho·∫∑c kh√¥ng, kh√¥ng ph·∫£i l√† s·ªë √¢m
  processed = processed.replace(/(\s+-\s+|(?<!\d)-(?!\d))/g, ", ");

  // 2. Thay th·∫ø c√°c t·ª´ vi·∫øt t·∫Øt d·ª±a tr√™n t·ª´ ƒëi·ªÉn
  // S·ª≠ d·ª•ng Regex v·ªõi bi√™n t·ª´ (\b) ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng thay th·∫ø nh·∫ßm (v√≠ d·ª• kh√¥ng thay th·∫ø ch·ªØ trong t·ª´ kh√°c)
  Object.entries(VIETNAMESE_ABBREVIATIONS).forEach(([abbr, fullText]) => {
    // Flag 'g' ƒë·ªÉ thay th·∫ø t·∫•t c·∫£, 'i' ƒë·ªÉ kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng (t√πy ch·ªçn, ·ªü ƒë√¢y ta ∆∞u ti√™n kh·ªõp ch√≠nh x√°c ho·∫∑c linh ho·∫°t)
    // V·ªõi c√°c t·ª´ vi·∫øt t·∫Øt h√†nh ch√≠nh, th∆∞·ªùng l√† vi·∫øt hoa, nh∆∞ng user c√≥ th·ªÉ vi·∫øt th∆∞·ªùng.
    // Ta d√πng 'i' (insensitive) ƒë·ªÉ h·ªó tr·ª£ t·ªët nh·∫•t.
    const regex = new RegExp(`\\b${abbr}\\b`, 'gi');
    processed = processed.replace(regex, fullText);
  });

  // 3. X·ª≠ l√Ω kho·∫£ng tr·∫Øng th·ª´a
  processed = processed.replace(/\s+/g, ' ').trim();

  return processed;
};

/**
 * C·∫Øt √¢m thanh m·∫´u ch·ªâ l·∫•y 20 gi√¢y ƒë·∫ßu ti√™n ƒë·ªÉ ph√¢n t√≠ch.
 */
const trimAudioTo20Seconds = async (audioArrayBuffer: ArrayBuffer): Promise<{ base64: string, duration: number }> => {
  try {
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    const audioBuffer = await audioContext.decodeAudioData(audioArrayBuffer.slice(0));
    
    const durationToKeep = Math.min(audioBuffer.duration, 20);
    const sampleRate = audioBuffer.sampleRate;
    const framesToKeep = Math.floor(durationToKeep * sampleRate);
    
    const newBuffer = audioContext.createBuffer(audioBuffer.numberOfChannels, framesToKeep, sampleRate);
    
    for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
      newBuffer.getChannelData(i).set(audioBuffer.getChannelData(i).slice(0, framesToKeep));
    }

    const wavBlob = pcmToWav(audioBufferToWav(newBuffer), sampleRate);
    const reader = new FileReader();
    
    return new Promise((resolve) => {
      reader.onloadend = () => {
        const base64 = (reader.result as string).split(',')[1];
        resolve({ base64, duration: durationToKeep });
      };
      reader.readAsDataURL(wavBlob);
    });
  } catch (e) {
    const blob = new Blob([audioArrayBuffer], { type: 'audio/wav' });
    const reader = new FileReader();
    return new Promise((resolve) => {
        reader.onloadend = () => resolve({ base64: (reader.result as string).split(',')[1], duration: 0 });
        reader.readAsDataURL(blob);
    });
  }
};

export function audioBufferToWav(buffer: AudioBuffer): ArrayBuffer {
  const numChannels = buffer.numberOfChannels;
  const length = buffer.length * numChannels * 2;
  const result = new ArrayBuffer(length);
  const view = new DataView(result);
  const channels = [];
  for (let i = 0; i < numChannels; i++) {
    channels.push(buffer.getChannelData(i));
  }
  let offset = 0;
  for (let i = 0; i < buffer.length; i++) {
    for (let channel = 0; channel < numChannels; channel++) {
      let sample = channels[channel][i];
      sample = Math.max(-1, Math.min(1, sample));
      view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
      offset += 2;
    }
  }
  return result;
}

export const mixAudio = async (speechBuffer: ArrayBuffer, musicBuffer: ArrayBuffer, musicVolume: number): Promise<ArrayBuffer> => {
  const ctx = new (window.AudioContext || (window as any).webkitAudioContext)();
  
  // Decode both buffers
  // Note: decodeAudioData detaches the buffer, so we slice it to keep the original safe if needed elsewhere
  const speech = await ctx.decodeAudioData(speechBuffer.slice(0));
  const music = await ctx.decodeAudioData(musicBuffer.slice(0));

  // Create OfflineAudioContext with the duration of the speech
  const offlineCtx = new OfflineAudioContext(1, speech.length, speech.sampleRate);

  // 1. Setup Speech Source
  const speechSource = offlineCtx.createBufferSource();
  speechSource.buffer = speech;
  speechSource.connect(offlineCtx.destination);
  speechSource.start(0);

  // 2. Setup Music Source (Looping to fit speech duration)
  const musicSource = offlineCtx.createBufferSource();
  musicSource.buffer = music;
  musicSource.loop = true; // Loop background music
  
  // 3. Apply Volume to Music
  const gainNode = offlineCtx.createGain();
  gainNode.gain.value = musicVolume; // 0.0 to 1.0
  
  musicSource.connect(gainNode);
  gainNode.connect(offlineCtx.destination);
  musicSource.start(0);

  // 4. Render
  const renderedBuffer = await offlineCtx.startRendering();
  
  // 5. Convert back to WAV ArrayBuffer
  return audioBufferToWav(renderedBuffer);
};

export const generateContentFromDescription = async (
  prompt: string, 
  modePrompt: string, 
  onLog?: (m: string, t: 'info' | 'error') => void,
  apiKey: string = process.env.API_KEY || ""
) => {
  onLog?.("Gemini ƒëang so·∫°n th·∫£o n·ªôi dung...", "info");
  try {
    const ai = new GoogleGenAI({ apiKey });
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: `${modePrompt}\n\nY√™u c·∫ßu: ${prompt}`,
    });
    return response.text || '';
  } catch (error: any) {
    const errorInfo = handleAiError(error);
    onLog?.(errorInfo.message, 'error');
    throw new Error(errorInfo.message);
  }
};

export const generateMarketingContent = async (
  imageBase64: string | null,
  description: string,
  onLog?: (m: string, t: 'info' | 'error') => void,
  apiKey: string = process.env.API_KEY || ""
): Promise<{title: string, content: string}> => {
  onLog?.("Gemini ƒëang ph√¢n t√≠ch v√† vi·∫øt qu·∫£ng c√°o...", "info");
  try {
    const ai = new GoogleGenAI({ apiKey });
    const parts: any[] = [];
    
    if (imageBase64) {
      parts.push({ inlineData: { data: imageBase64, mimeType: 'image/jpeg' } });
    }
    
    parts.push({ 
      text: `B·∫°n l√† chuy√™n gia Marketing. H√£y vi·∫øt n·ªôi dung qu·∫£ng c√°o ng·∫Øn g·ªçn, gi·∫≠t g√¢n, thu h√∫t cho s·∫£n ph·∫©m/d·ªãch v·ª• n√†y.
      ${description ? `M√¥ t·∫£ th√™m: ${description}` : ''}
      
      Y√™u c·∫ßu tr·∫£ v·ªÅ JSON v·ªõi ƒë·ªãnh d·∫°ng:
      {
        "title": "Ti√™u ƒë·ªÅ ng·∫Øn (d∆∞·ªõi 10 t·ª´), vi·∫øt hoa, g√¢y s·ªëc ho·∫∑c t√≤ m√≤",
        "content": "N·ªôi dung ch√≠nh (d∆∞·ªõi 50 t·ª´), k√™u g·ªçi h√†nh ƒë·ªông m·∫°nh m·∫Ω, d√πng icon emoji."
      }` 
    });

    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: { parts },
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            title: { type: Type.STRING },
            content: { type: Type.STRING }
          },
          required: ["title", "content"]
        }
      }
    });

    return JSON.parse(response.text || '{"title": "Qu·∫£ng c√°o", "content": "N·ªôi dung ƒëang c·∫≠p nh·∫≠t..."}');
  } catch (error: any) {
    const errorInfo = handleAiError(error);
    onLog?.(errorInfo.message, 'error');
    throw new Error(errorInfo.message);
  }
};

export const analyzeVoice = async (
  rawAudioBuffer: ArrayBuffer, 
  onLog?: (m: string, t: 'info' | 'error') => void,
  apiKey: string = process.env.API_KEY || ""
): Promise<any> => {
  onLog?.("ƒêang chu·∫©n b·ªã 20 gi√¢y √¢m thanh m·∫´u ƒë·ªÉ ph√¢n t√≠ch...", "info");
  const { base64 } = await trimAudioTo20Seconds(rawAudioBuffer);

  onLog?.("ƒêang t·∫°o ƒë·ªô tr·ªÖ an to√†n (3s)...", "info");
  await delay(3000);
  
  onLog?.("ƒêang ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm gi·ªçng n√≥i...", "info");

  try {
    const ai = new GoogleGenAI({ apiKey });
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview', 
      contents: {
        parts: [
          { inlineData: { data: base64, mimeType: 'audio/wav' } },
          { text: `Analyze this audio. Return JSON: gender ("Nam"/"N·ªØ"), region ("B·∫Øc"/"Trung"/"Nam"), toneSummary (5 words), suggestedName (Vietnamese), description.` }
        ]
      },
      config: { 
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            gender: { type: Type.STRING, enum: ["Nam", "N·ªØ"] },
            region: { type: Type.STRING, enum: ["B·∫Øc", "Trung", "Nam", "Kh√°c"] },
            toneSummary: { type: Type.STRING },
            suggestedName: { type: Type.STRING },
            description: { type: Type.STRING }
          },
          required: ["gender", "region", "toneSummary", "suggestedName", "description"]
        }
      }
    });
    return JSON.parse(response.text || "{}");
  } catch (error: any) {
    const errorInfo = handleAiError(error);
    onLog?.(errorInfo.message, 'error');
    throw new Error(errorInfo.message);
  }
};

export const generateAudioSegment = async (
  text: string, 
  config: any,
  onLog?: (m: string, t: 'info' | 'error') => void,
  apiKey: string = process.env.API_KEY || ""
): Promise<ArrayBuffer> => {
  try {
    const ai = new GoogleGenAI({ apiKey });
    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text }] }],
      config: {
        responseModalities: [Modality.AUDIO],
        speechConfig: {
          voiceConfig: { prebuiltVoiceConfig: { voiceName: config.voiceName } },
        },
      },
    });

    const base64 = response.candidates?.[0]?.content?.parts?.find(p => p.inlineData)?.inlineData?.data;
    if (!base64) throw new Error("Google AI kh√¥ng ph·∫£n h·ªìi √¢m thanh.");
    
    const binaryString = atob(base64);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) bytes[i] = binaryString.charCodeAt(i);
    return bytes.buffer;
  } catch (error: any) {
    const errorInfo = handleAiError(error);
    onLog?.(errorInfo.message, 'error');
    throw new Error(errorInfo.message);
  }
};

export const generateAudioParallel = async (
  text: string,
  config: any,
  onProgress: (percent: number) => void,
  onLog?: (m: string, t: 'info' | 'error') => void,
  apiKey: string = process.env.API_KEY || ""
): Promise<ArrayBuffer> => {
  // 1. Chu·∫©n h√≥a vƒÉn b·∫£n: Bung c√°c t·ª´ vi·∫øt t·∫Øt & x·ª≠ l√Ω d·∫•u c√¢u
  const normalizedText = normalizeTextForSpeech(text);
  
  if (text !== normalizedText) {
    onLog?.("ƒê√£ t·ª± ƒë·ªông m·ªü r·ªông c√°c t·ª´ vi·∫øt t·∫Øt ƒë·ªÉ ƒë·ªçc r√µ r√†ng h∆°n.", "info");
  }

  // 2. Chia ƒëo·∫°n vƒÉn b·∫£n ƒë√£ chu·∫©n h√≥a
  const rawChunks = normalizedText.match(/[^.!?\n]+[.!?\n]*|[^.!?\n]+/g) || [normalizedText];
  const combinedChunks: string[] = [];
  let current = "";
  const SAFE_CHAR_LIMIT = 1200; 

  for (const c of rawChunks) {
    if ((current + c).length < SAFE_CHAR_LIMIT) {
      current += c;
    } else {
      if (current) combinedChunks.push(current.trim());
      current = c;
    }
  }
  if (current) combinedChunks.push(current.trim());

  const total = combinedChunks.length;
  onLog?.(`Kh·ªüi t·∫°o chuy·ªÉn ƒë·ªïi (${total} ph√¢n ƒëo·∫°n)...`, 'info');
  const results: ArrayBuffer[] = [];
  
  for (let i = 0; i < total; i++) {
    if (i > 0) {
        onLog?.(`Ch·ªù 3s ƒë·ªÉ x·ª≠ l√Ω ƒëo·∫°n ti·∫øp theo (${i + 1}/${total})...`, 'info');
        // TƒÉng ƒë·ªô tr·ªÖ l√™n 3000ms ƒë·ªÉ an to√†n h∆°n cho c√°c t√†i kho·∫£n free
        await delay(3000);
    }
    
    const segment = await generateAudioSegment(combinedChunks[i], config, onLog, apiKey);
    results.push(segment);
    onProgress(Math.round(((i + 1) / total) * 100));
  }

  const totalLength = results.reduce((acc, b) => acc + b.byteLength, 0);
  const finalBuffer = new Uint8Array(totalLength);
  let offset = 0;
  for (const res of results) {
    finalBuffer.set(new Uint8Array(res), offset);
    offset += res.byteLength;
  }
  return finalBuffer.buffer;
};

export const pcmToWav = (pcmBuffer: ArrayBuffer, sampleRate: number = 24000): Blob => {
  const length = pcmBuffer.byteLength;
  const buffer = new ArrayBuffer(44 + length);
  const view = new DataView(buffer);
  view.setUint32(0, 0x52494646, false); 
  view.setUint32(4, 36 + length, true); 
  view.setUint32(8, 0x57415645, false); 
  view.setUint32(12, 0x666d7420, false); 
  view.setUint16(20, 1, true);           
  view.setUint16(22, 1, true);           
  view.setUint32(24, sampleRate, true);  
  view.setUint32(28, sampleRate * 2, true); 
  view.setUint16(32, 2, true);           
  view.setUint16(34, 16, true);          
  view.setUint32(36, 0x64617461, false); 
  view.setUint32(40, length, true);      
  new Uint8Array(buffer, 44).set(new Uint8Array(pcmBuffer));
  return new Blob([buffer], { type: 'audio/wav' });
};

export const pcmToMp3 = (pcmBuffer: ArrayBuffer, sampleRate: number = 24000): Blob => {
  const lamejs = (window as any).lamejs;
  if (!lamejs || !lamejs.Mp3Encoder) return pcmToWav(pcmBuffer, sampleRate);
  const mp3encoder = new lamejs.Mp3Encoder(1, sampleRate, 128); 
  const samples = new Int16Array(pcmBuffer);
  const mp3Data = [];
  for (let i = 0; i < samples.length; i += 1152) {
    const chunk = samples.subarray(i, i + 1152);
    const mp3buf = mp3encoder.encodeBuffer(chunk);
    if (mp3buf.length > 0) mp3Data.push(mp3buf);
  }
  const final = mp3encoder.flush();
  if (final.length > 0) mp3Data.push(final);
  return new Blob(mp3Data, { type: 'audio/mp3' });
};